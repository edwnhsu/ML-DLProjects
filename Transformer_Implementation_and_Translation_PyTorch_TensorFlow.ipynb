{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a45abd-69ef-40ae-a5c7-554383dd45de",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "### 2024-07-31\n",
    "### Yuwei Hsu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869f217-b993-4148-b897-7e238681d03b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> This code file is ONLY for Question 1 in Assignment 4 .\n",
    "For Question 2, please refer to the other file provided in the upload session.\n",
    "\n",
    "Thank you.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b5436-fad7-4407-a8d8-50882320e958",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "037becd6-6111-44b1-a42d-232655575d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianTokenizer, get_linear_schedule_with_warmup\n",
    "from sacrebleu.metrics import BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6790307-f383-4829-bc82-80f9fd081974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers, processors\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d48d21-1e8b-4edb-ab5e-a295ca4f223e",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9875b92f-1cc5-4d10-830d-e7ec8b3e3551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae1f65a9a6747cd964cc244535bba1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pre-trained tokenizers...\n",
      "Encoding training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding data: 100% 100/100 [00:12<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding data: 100% 1/1 [00:00<00:00, 13.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training batches: 3125\n",
      "Number of validation batches: 32\n",
      "\n",
      "Sample batch shape:\n",
      "Input IDs: torch.Size([32, 128])\n",
      "Attention Mask: torch.Size([32, 128])\n",
      "Labels: torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data Preprocessing\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "def encode_translation_data(data, tokenizer_src, tokenizer_tgt, max_length, batch_size=1000):\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(data), batch_size), desc=\"Encoding data\"):\n",
    "        batch = data.select(range(i, min(i + batch_size, len(data))))\n",
    "        translations = batch['translation']\n",
    "        src_texts = [item['en'] for item in translations]\n",
    "        tgt_texts = [item['fr'] for item in translations]\n",
    "        \n",
    "        src_encodings = tokenizer_src(src_texts, padding='max_length', truncation=True, max_length=max_length)\n",
    "        tgt_encodings = tokenizer_tgt(tgt_texts, padding='max_length', truncation=True, max_length=max_length)\n",
    "        \n",
    "        all_input_ids.extend(src_encodings['input_ids'])\n",
    "        all_attention_masks.extend(src_encodings['attention_mask'])\n",
    "        all_labels.extend(tgt_encodings['input_ids'])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_masks,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "def preprocess_data(train_size, valid_size, max_length, batch_size):\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(\"wmt14\", \"fr-en\")\n",
    "    \n",
    "    # Select training and validation data\n",
    "    train_data = dataset['train'].select(range(train_size))\n",
    "    valid_data = dataset['validation'].select(range(valid_size))\n",
    "    \n",
    "    # Load pre-trained tokenizers\n",
    "    print(\"\\nLoading pre-trained tokenizers...\")\n",
    "    tokenizer_src = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_tgt = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "    \n",
    "    # Encode data\n",
    "    print(\"Encoding training data...\")\n",
    "    train_encodings = encode_translation_data(train_data, tokenizer_src, tokenizer_tgt, max_length)\n",
    "    print(\"Encoding validation data...\")\n",
    "    valid_encodings = encode_translation_data(valid_data, tokenizer_src, tokenizer_tgt, max_length)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TranslationDataset(train_encodings)\n",
    "    valid_dataset = TranslationDataset(valid_encodings)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    train_size = 100000\n",
    "    valid_size = 1000\n",
    "    max_length = 128\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_loader, valid_loader, tokenizer_src, tokenizer_tgt = preprocess_data(train_size, valid_size, max_length, batch_size)\n",
    "    \n",
    "    print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(valid_loader)}\")\n",
    "    \n",
    "    # Check the first batch of data\n",
    "    for batch in train_loader:\n",
    "        print(\"\\nSample batch shape:\")\n",
    "        print(f\"Input IDs: {batch['input_ids'].shape}\")\n",
    "        print(f\"Attention Mask: {batch['attention_mask'].shape}\")\n",
    "        print(f\"Labels: {batch['labels'].shape}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d939dac-de6d-4ff2-b9a2-f0d1863f67c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking English Tokenizer:\n",
      "Tokenizer vocabulary size: 30522\n",
      "Tokenizer name: bert-base-uncased\n",
      "\n",
      "Tokenization Examples:\n",
      "\n",
      "Original: I'm fine thank you\n",
      "Encoded token IDs: [101, 1045, 1005, 1049, 2986, 4067, 2017, 102]\n",
      "Actual tokens: ['[CLS]', 'i', \"'\", 'm', 'fine', 'thank', 'you', '[SEP]']\n",
      "Decoded: [CLS] i'm fine thank you [SEP]\n",
      "\n",
      "Original: Hello, how are you doing today?\n",
      "Encoded token IDs: [101, 7592, 1010, 2129, 2024, 2017, 2725, 2651, 1029, 102]\n",
      "Actual tokens: ['[CLS]', 'hello', ',', 'how', 'are', 'you', 'doing', 'today', '?', '[SEP]']\n",
      "Decoded: [CLS] hello, how are you doing today? [SEP]\n",
      "\n",
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "Encoded token IDs: [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102]\n",
      "Actual tokens: ['[CLS]', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', '[SEP]']\n",
      "Decoded: [CLS] the quick brown fox jumps over the lazy dog. [SEP]\n",
      "\n",
      "Original: In machine learning, tokenization is a fundamental preprocessing step.\n",
      "Encoded token IDs: [101, 1999, 3698, 4083, 1010, 19204, 3989, 2003, 1037, 8050, 17463, 3217, 9623, 7741, 3357, 1012, 102]\n",
      "Actual tokens: ['[CLS]', 'in', 'machine', 'learning', ',', 'token', '##ization', 'is', 'a', 'fundamental', 'prep', '##ro', '##ces', '##sing', 'step', '.', '[SEP]']\n",
      "Decoded: [CLS] in machine learning, tokenization is a fundamental preprocessing step. [SEP]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Checking French Tokenizer:\n",
      "Tokenizer vocabulary size: 32005\n",
      "Tokenizer name: camembert-base\n",
      "\n",
      "Tokenization Examples:\n",
      "\n",
      "Original: Je vais bien merci\n",
      "Encoded token IDs: [5, 100, 676, 72, 895, 6]\n",
      "Actual tokens: ['<s>', '▁Je', '▁vais', '▁bien', '▁merci', '</s>']\n",
      "Decoded: <s> Je vais bien merci</s>\n",
      "\n",
      "Original: Bonjour, comment allez-vous aujourd'hui ?\n",
      "Encoded token IDs: [5, 1285, 7, 404, 1513, 26, 315, 405, 11, 265, 106, 6]\n",
      "Actual tokens: ['<s>', '▁Bonjour', ',', '▁comment', '▁allez', '-', 'vous', '▁aujourd', \"'\", 'hui', '▁?', '</s>']\n",
      "Decoded: <s> Bonjour, comment allez-vous aujourd'hui?</s>\n",
      "\n",
      "Original: Le renard brun rapide saute par-dessus le chien paresseux.\n",
      "Encoded token IDs: [5, 54, 19376, 6529, 924, 13600, 37, 26, 1364, 16, 1877, 30583, 9, 6]\n",
      "Actual tokens: ['<s>', '▁Le', '▁renard', '▁brun', '▁rapide', '▁saute', '▁par', '-', 'dessus', '▁le', '▁chien', '▁paresseux', '.', '</s>']\n",
      "Decoded: <s> Le renard brun rapide saute par-dessus le chien paresseux.</s>\n",
      "\n",
      "Original: En apprentissage automatique, la tokenisation est une étape de prétraitement fondamentale.\n",
      "Encoded token IDs: [5, 107, 6965, 3314, 7, 13, 1200, 6840, 1385, 30, 28, 2131, 8, 790, 27570, 10098, 9, 6]\n",
      "Actual tokens: ['<s>', '▁En', '▁apprentissage', '▁automatique', ',', '▁la', '▁to', 'ken', 'isation', '▁est', '▁une', '▁étape', '▁de', '▁pré', 'traitement', '▁fondamentale', '.', '</s>']\n",
      "Decoded: <s> En apprentissage automatique, la tokenisation est une étape de prétraitement fondamentale.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def check_tokenization(tokenizer, sample_texts):\n",
    "    print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Tokenizer name: {tokenizer.name_or_path}\")\n",
    "    print(\"\\nTokenization Examples:\")\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        print(f\"\\nOriginal: {text}\")\n",
    "        \n",
    "        # Encode the text\n",
    "        encoded = tokenizer.encode(text, add_special_tokens=True)\n",
    "        \n",
    "        # Get the actual tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded)\n",
    "        \n",
    "        # Decode back to text\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        \n",
    "        print(f\"Encoded token IDs: {encoded}\")\n",
    "        print(f\"Actual tokens: {tokens}\")\n",
    "        print(f\"Decoded: {decoded}\")\n",
    "        \n",
    "        # Check if any unknown tokens were used\n",
    "        unknown_tokens = [token for token in tokens if token == tokenizer.unk_token]\n",
    "        if unknown_tokens:\n",
    "            print(f\"Warning: {len(unknown_tokens)} unknown tokens found.\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Load English tokenizer\n",
    "    tokenizer_src = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Load French tokenizer\n",
    "    tokenizer_tgt = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "    \n",
    "    # Prepare some sample texts\n",
    "    english_samples = [\n",
    "        \"I'm fine thank you\",\n",
    "        \"Hello, how are you doing today?\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"In machine learning, tokenization is a fundamental preprocessing step.\"\n",
    "    ]\n",
    "    \n",
    "    french_samples = [\n",
    "        \"Je vais bien merci\",\n",
    "        \"Bonjour, comment allez-vous aujourd'hui ?\",\n",
    "        \"Le renard brun rapide saute par-dessus le chien paresseux.\",\n",
    "        \"En apprentissage automatique, la tokenisation est une étape de prétraitement fondamentale.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Checking English Tokenizer:\")\n",
    "    check_tokenization(tokenizer_src, english_samples)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"Checking French Tokenizer:\")\n",
    "    check_tokenization(tokenizer_tgt, french_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537adb0f-99df-4fde-b731-26bfd00fb9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf67e1-c205-47e5-b8a4-3116e23f7c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c785fc9-0be7-4fb6-b599-357290fd3a56",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9777ef91-f3b1-4865-ad80-04875f8da028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = self.dropout(torch.softmax(scores, dim=-1))\n",
    "        \n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embed(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        x = self.embed(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return x\n",
    "\n",
    "# Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout)\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        output = self.linear(dec_output)\n",
    "        return output\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, src_mask, tgt_mask):\n",
    "        return self.decoder(tgt, memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3ef7a1-3ffb-4b55-b06e-815df12bb11f",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00d98d06-25e5-4bad-a2a9-71a3acda5504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create masks\n",
    "def create_mask(src, tgt, pad_token_id):\n",
    "    src_mask = (src != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_mask = (tgt != pad_token_id).unsqueeze(1).unsqueeze(3)\n",
    "    seq_length = tgt.size(1)\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "    tgt_mask = tgt_mask & nopeak_mask.to(tgt.device)\n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "# Train for one epoch\n",
    "def train_epoch(model, data_loader, optimizer, criterion, pad_token_id, device, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    progress_bar = tqdm(total=len(data_loader), desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        src = batch['input_ids'].to(device)\n",
    "        tgt = batch['labels'].to(device)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        src_mask, tgt_mask = create_mask(src, tgt_input, pad_token_id)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        \n",
    "        output = output.contiguous().view(-1, output.size(-1))\n",
    "        tgt_output = tgt_output.contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt_output)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\", 'lr': f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return avg_loss, elapsed_time\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, data_loader, criterion, pad_token_id, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            src = batch['input_ids'].to(device)\n",
    "            tgt = batch['labels'].to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            src_mask, tgt_mask = create_mask(src, tgt_input, pad_token_id)\n",
    "            \n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "            \n",
    "            output = output.contiguous().view(-1, output.size(-1))\n",
    "            tgt_output = tgt_output.contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Train the Transformer model\n",
    "def train_transformer(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs, device, pad_token_id):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            src = batch['input_ids'].to(device)\n",
    "            tgt = batch['labels'].to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            src_mask, tgt_mask = create_mask(src, tgt_input, pad_token_id)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "            loss = criterion(output.contiguous().view(-1, output.size(-1)), tgt_output.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                src = batch['input_ids'].to(device)\n",
    "                tgt = batch['labels'].to(device)\n",
    "                tgt_input = tgt[:, :-1]\n",
    "                tgt_output = tgt[:, 1:]\n",
    "\n",
    "                src_mask, tgt_mask = create_mask(src, tgt_input, pad_token_id)\n",
    "\n",
    "                output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "                loss = criterion(output.contiguous().view(-1, output.size(-1)), tgt_output.contiguous().view(-1))\n",
    "\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}\")\n",
    "\n",
    "        if avg_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            torch.save(model.state_dict(), f'best_model_epoch_{epoch+1}.pt')\n",
    "            print(f\"New best model saved with validation loss: {best_valid_loss:.4f}\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr_scheduler(optimizer, d_model, warmup_steps):\n",
    "    def lr_lambda(step):\n",
    "        step = max(1, step)  # Avoid division by zero\n",
    "        return min(step ** (-0.5), step * (warmup_steps ** (-1.5))) * (d_model ** (-0.5))\n",
    "    \n",
    "    return LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d025c-6281-4528-9ac4-f69d35990d3f",
   "metadata": {},
   "source": [
    "### Train transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "533cf843-6fb9-4d89-b714-e66be18cb599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831cc88ba95348e58c1c208f65465492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pre-trained tokenizers...\n",
      "Encoding training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding data: 100% 100/100 [00:12<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding data: 100% 1/1 [00:00<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training batches: 3125\n",
      "Number of validation batches: 32\n",
      "\n",
      "Sample batch shape:\n",
      "Input IDs: torch.Size([32, 128])\n",
      "Attention Mask: torch.Size([32, 128])\n",
      "Labels: torch.Size([32, 128])\n",
      "Source vocabulary size: 30522\n",
      "Target vocabulary size: 32004\n",
      "Pad token ID: 0\n",
      "Device: cuda\n",
      "Model parameters: 92569860\n",
      "Initial learning rate: 1.746928107421711e-11\n",
      "Number of epochs: 2\n",
      "Warmup steps: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training: 100% 3125/3125 [28:58<00:00,  1.80it/s]\n",
      "Epoch 1/2 - Validation: 100% 32/32 [00:05<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 8.3708, Valid Loss: 4.1679\n",
      "New best model saved with validation loss: 4.1679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Training: 100% 3125/3125 [28:57<00:00,  1.80it/s]\n",
      "Epoch 2/2 - Validation: 100% 32/32 [00:05<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 3.4544, Valid Loss: 2.1944\n",
      "New best model saved with validation loss: 2.1944\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model_class, path, *args, **kwargs):\n",
    "    model = model_class(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Training script\n",
    "if __name__ == \"__main__\":\n",
    "    # Data preprocessing\n",
    "    train_size = 100000\n",
    "    valid_size = 1000\n",
    "    max_length = 128\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_loader, valid_loader, tokenizer_src, tokenizer_tgt = preprocess_data(train_size, valid_size, max_length, batch_size)\n",
    "    \n",
    "    print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(valid_loader)}\")\n",
    "    \n",
    "    # Check the first batch of data\n",
    "    for batch in train_loader:\n",
    "        print(\"\\nSample batch shape:\")\n",
    "        print(f\"Input IDs: {batch['input_ids'].shape}\")\n",
    "        print(f\"Attention Mask: {batch['attention_mask'].shape}\")\n",
    "        print(f\"Labels: {batch['labels'].shape}\")\n",
    "        break\n",
    "\n",
    "    # Initialize tokenizers\n",
    "    tokenizer_src = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_tgt = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "\n",
    "    # Get vocabulary size\n",
    "    src_vocab_size = len(tokenizer_src.vocab)\n",
    "    tgt_vocab_size = len(tokenizer_tgt.vocab)\n",
    "\n",
    "    # Set other parameters\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    max_seq_length = 128\n",
    "    dropout = 0.1\n",
    "\n",
    "    num_epochs = 2\n",
    "\n",
    "    # Get pad token ID\n",
    "    pad_token_id = tokenizer_src.pad_token_id\n",
    "\n",
    "    # Ensure using the correct device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout).to(device)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    # Initialize the loss function\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id, reduction='mean')\n",
    "\n",
    "    warmup_steps = 4000\n",
    "    scheduler = get_lr_scheduler(optimizer, d_model, warmup_steps)\n",
    "\n",
    "    print(f\"Source vocabulary size: {src_vocab_size}\")\n",
    "    print(f\"Target vocabulary size: {tgt_vocab_size}\")\n",
    "    print(f\"Pad token ID: {pad_token_id}\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    print(f\"Initial learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    print(f\"Number of epochs: {num_epochs}\")\n",
    "    print(f\"Warmup steps: {warmup_steps}\")\n",
    "\n",
    "    # Train the model\n",
    "    train_transformer(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs, device, pad_token_id)\n",
    "\n",
    "    # Save the trained model\n",
    "    save_model(model, 'transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bfa022-c0b3-443b-a8be-26dedc21ac46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3376e06a-b2f1-4c1a-9be9-19359522c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, CamembertTokenizerFast\n",
    "\n",
    "# Define the Transformer model and necessary components here (or import them if they are in a separate file)\n",
    "# Positional Encoding, MultiHeadAttention, FeedForward, EncoderLayer, DecoderLayer, Encoder, Decoder, Transformer classes remain the same...\n",
    "\n",
    "# Mask creation, tokenization, and greedy decoding functions\n",
    "def tokenize_input(sentence, tokenizer, max_length):\n",
    "    tokens = tokenizer(sentence, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
    "    return tokens.input_ids, tokens.attention_mask\n",
    "\n",
    "def create_src_mask(src, pad_token_id):\n",
    "    return (src != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_length, start_token_id, end_token_id, pad_token_id):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    ys = torch.ones(1, 1).fill_(start_token_id).type_as(src.data).to(device)\n",
    "    \n",
    "    for i in range(max_length - 1):\n",
    "        tgt_mask = create_src_mask(ys, pad_token_id).to(device)\n",
    "        out = model.decode(ys, memory, src_mask, tgt_mask)\n",
    "        prob = model.linear(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        \n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word).to(device)], dim=1)\n",
    "        \n",
    "        if next_word == end_token_id:\n",
    "            break\n",
    "    \n",
    "    return ys\n",
    "\n",
    "def translate_sentence(model, sentence, tokenizer_src, tokenizer_tgt, max_length, device, pad_token_id):\n",
    "    model.eval()\n",
    "    src, src_mask = tokenize_input(sentence, tokenizer_src, max_length)\n",
    "    src_mask = create_src_mask(src, pad_token_id)\n",
    "    start_token_id = tokenizer_tgt.cls_token_id\n",
    "    end_token_id = tokenizer_tgt.sep_token_id\n",
    "    \n",
    "    translated_tokens = greedy_decode(model, src, src_mask, max_length, start_token_id, end_token_id, pad_token_id)\n",
    "    \n",
    "    translated_sentence = tokenizer_tgt.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_sentence\n",
    "\n",
    "# Load the model and perform inference\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    max_length = 128\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize tokenizers\n",
    "    tokenizer_src = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_tgt = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "\n",
    "    # Get vocabulary size\n",
    "    src_vocab_size = len(tokenizer_src.vocab)\n",
    "    tgt_vocab_size = len(tokenizer_tgt.vocab)\n",
    "\n",
    "    # Set model parameters\n",
    "    d_model = 512\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    dropout = 0.1\n",
    "\n",
    "    # Get pad token ID\n",
    "    pad_token_id = tokenizer_src.pad_token_id\n",
    "\n",
    "    # Load the model\n",
    "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_length, dropout).to(device)\n",
    "    model.load_state_dict(torch.load('transformer_model.pth'))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Translate a sample sentence\n",
    "    input_sentence = \"Hello, how are you?\"\n",
    "    translated_sentence = translate_sentence(model, input_sentence, tokenizer_src, tokenizer_tgt, max_length, device, pad_token_id)\n",
    "    print(f\"Translated sentence: {translated_sentence}\")\n",
    "\n",
    "    # Debugging information\n",
    "    print(\"\\nDebugging Information:\")\n",
    "    print(f\"Input Sentence: {input_sentence}\")\n",
    "    print(f\"Source Tokens: {tokenizer_src.encode(input_sentence)}\")\n",
    "    print(f\"Translated Tokens: {translated_sentence}\")\n",
    "\n",
    "    # Check special tokens\n",
    "    print(f\"Start token ID: {tokenizer_tgt.cls_token_id}\")\n",
    "    print(f\"End token ID: {tokenizer_tgt.sep_token_id}\")\n",
    "    print(f\"Pad token ID: {pad_token_id}\")\n",
    "\n",
    "    # Print each decoding step's tokens\n",
    "    src, src_mask = tokenize_input(input_sentence, tokenizer_src, max_length)\n",
    "    src_mask = create_src_mask(src, pad_token_id)\n",
    "    start_token_id = tokenizer_tgt.cls_token_id\n",
    "    end_token_id = tokenizer_tgt.sep_token_id\n",
    "\n",
    "    translated_tokens = greedy_decode(model, src, src_mask, max_length, start_token_id, end_token_id, pad_token_id)\n",
    "    print(f\"Decoded Token IDs: {translated_tokens[0].tolist()}\")\n",
    "    print(f\"Decoded Tokens: {tokenizer_tgt.convert_ids_to_tokens(translated_tokens[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21536af1-bfd3-42bd-9ffe-091c918923bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d921f9-ff89-42f5-be50-6b379d891490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7475bc-33ea-4f7e-b7f2-43dd2c023745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52bfa7-95c9-41ae-8043-ed381fce734e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2aa46c-8249-4fc2-9e47-7143feaad983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e8c03-337b-443d-bee9-6254bc1aeb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbdb92-38d1-4571-86fa-2780fec3aca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a6861-f96a-429a-aafc-24016d3d5313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad72d8-784f-43c1-b017-ca4a93cb171c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33584765-c8d0-4c0d-b94a-d2b4355f844e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a8cd2-60e8-4a73-b4f5-9e04118b3354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c3f36-0d58-4224-98ac-93f9d4f301e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed798a8d-7813-48ca-9a3b-22693c1cae9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53315a98-1e77-42f6-b829-1b3f67d4388d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc81ea-32ed-4abb-8a1c-c8c5a25f0f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21569354-3fcb-4bcb-8593-36c400982f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
